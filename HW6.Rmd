---
title: "Hw6"
author: "Arjun"
date: "November 29, 2018"
output:
  word_document: default
  html_document: default
---

```{r}
library(randomForest)
library(ISLR)
library(dplyr)
```

```{r}
train = sample(392, 392/2)
trainset = Auto[train,]
testset = Auto[-train,]
```

```{r}
rf = randomForest(mpg ~ cylinders + displacement + horsepower + year + origin, data = trainset, mtry = 3, importance = TRUE)
rf
```

```{r}
yhat = predict(rf, newdata = trainset)
plot(yhat, testset$mpg)
abline(0,1)
mean((yhat-testset$mpg)^2)
```

8. The plot and the mean test error shows a model which does a very poor job of predicting the actual values of the data. The MSE is nearly five times as large as the mean value of the set, and the plot looks more like a random scatter than a line at x=y

```{r}
use = Auto %>% select(mpg,year,horsepower,weight)
mat = data.matrix(use)
km = kmeans(mat, 2, nstart = 20)
km
```

9. The K-means divided the data into two distinct groups. The first group of 236 cars has a lower mpg lower weight and lower cylinder count than the second group of about 156 cars. year is pretty similar. this shows that there is a distinct clustering of the data around large powerful cars that use a lot of gas, and smaller less powerful cars that have a higher mpg.

```{r}
apply(use,2, mean)
apply(use,2,var)
pr = prcomp(use, scale = TRUE)
```

```{r}
pr
summary(pr)
```

10. The proportion of variance is shown in the summary. The PCA shows that mpg and year are positively correlated and horsepower and weight are negatively correlated with each other. 

```{r}
hc = hclust(dist(mat), method = "complete")
hc
plot(hc, cex = .9)
```
11. The data is divided into two main clusters.With one of cluster being bigger than the other. Based on otherr plots it appears as if yeasr is not relevant while the other three variables play a big part in the separation of the data. 


```{r}
d = cutree(hc,2)
plot(mat[,c(1,3)], col = (d+ 1))
plot(mat[,c(1,2)], col = (d+1))
plot(mat[,c(1,4)], col = (d + 1))
plot(mat[,c(3,4)], col = (d+1))
```

```{r}
plot(mat[,c(1,3)], col = (km$cluster+ 1))
plot(mat[,c(1,2)], col = (km$cluster+1))
plot(mat[,c(1,4)], col = (km$cluster + 1))
plot(mat[,c(3,4)], col = (km$cluster+1))
```

12. The two methods produce extremely similar plots, which is because they both used distance as a way to measure similarity and clustered based on that. K-means is more top down, while hierarhcal clustering starts with small clusters and brings them into large ones. But the basic method of minimizing distance between cluster groups makes it so that the two methods result in similar plots. 