---
title: "Lab 11"
author: "Arjun"
date: "November 26, 2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(randomForest)
library(MASS)
```

```{r}
set.seed(1)
train=sample(1:nrow(Boston), nrow(Boston)/2)
boston.test=Boston[-train, "medv"]
```

```{r}
summary(Boston)
```

There are 13 predictors in the Boston data set

```{r}
bag.boston=randomForest(medv~., data=Boston, subset=train, mtry=13, importance=TRUE)
yhat.bag=predict(bag.boston, newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag-boston.test)^2)
```

The plot shows that many of the predicted values are close to their actual values, but with more than a few large outliers. However, the MSE is about 13 which is nealry half the mean value and greater than the IQR. This suggests that the model is not a very good predictor of the actual values.

```{r}
rf.boston=randomForest(medv~., data=Boston, subset=train, mtry=sqrt(13), importance=TRUE)
yhat.rf=predict(rf.boston, newdata=Boston[-train,])
plot(yhat.rf, boston.test)
abline(0, 1)
mean((yhat.rf-boston.test)^2)
```
4. The random forest produces a plot where the data is less tightly clustered arounf x=y, but it has less outliers. This leads to a slightly smaller MSE of about 12. however, that is sitll pretty large comapred to the data, so the model still isn't very good. 

5. The difference between bagging and a random forest is that bagging uses all 13 predictors for every single iteration of the bootstrap data, while the random forest uses a subset of the predictors of size m, in this case it sqrt(13). This reduces the variance of the model without increasing the bias.
 

```{r}
importance(bag.boston)
varImpPlot(bag.boston)
```
6. the value of the two variables show that the most important two varibales are lstat and rm. lstat has a decrease in node impurity of about 9500, while rm has a value of about 7000. A similar separation appears in percent increase in MSE. in this case rm is the most important and lstat is the second most important. 

7.
A data frame with 50 observations in the 50 states on 4 variables.
[,1] Murder            numeric Murder arrests (per 100,000)
[,2] Assault           numeric Assault arrests (per 100,000) 
[,3] UrbanPop          numeric Percent urban population
[,4] Rape              numeric Rape arrests (per 100,000)

8.
```{r}
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
```

We standardize the data so that our PCA is radically affected by differences in range. This way everything is scaled to its relative effects. 

9.
```{r}
pr.out=prcomp(USArrests, scale=TRUE)
pr.out
```

Through a PCA you can represent higher dimensional data iwth a lower dimension thus understanding how the data is clustered. You can use it to more easily compare the effect of multiple variable on how the data at hand is dispersed.

```{r}
biplot(pr.out, scale=0)
pr.var=pr.out$sdev^2
pve=pr.var/sum(pr.var)
pve
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0, 1), type='b')
```

10. We use the first two principal componenets because we are trying to represent the data in only two dimensions.

```{r}
set.seed(2)
x=matrix(rnorm(50*2), ncol=2)
x[1:25, 1]=x[1:25, 1]+3
x[1:25, 2]=x[1:25, 2]-4
km.out=kmeans(x, 2, nstart=20)
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", pch=20, cex=2)
```

11. What you can get form the output is how to best divide the data into two disitnct groups.

```{r}
hc.complete=hclust(dist(x), method="complete")
plot(hc.complete, main="Complete Linkage", xlab="", sub="", cex=0.9)
d=cutree(hc.complete, 2)
plot(x, col=(d+1), main="Hierarchical Clustering Plot")
```

12.
Because hierarchal clustering used complete as the method of dissimilarity, which means that the method is very simlar to reducing vairation betweeen the clusters.